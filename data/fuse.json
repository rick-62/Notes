{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"My Notes","n":0.707},"1":{"v":"\nI am using Dendron to store all of my notes. \n\nThis includes everything from courses & books, as well as general reference and useful information from videos & articles.\n\nGithub Actions is configured to republish the notes on each commit: \n\nhttps://rick-62.github.io/Notes/\n\n\n## Useful Dendron resources\n\nThis section contains useful links to related resources.\n\n- [Getting Started Guide](https://link.dendron.so/6b25)\n- [Discord](https://link.dendron.so/6b23)\n- [Home Page](https://wiki.dendron.so/)\n- [Github](https://link.dendron.so/6b24)\n- [Developer Docs](https://docs.dendron.so/)\n","n":0.13}}},{"i":2,"$":{"0":{"v":"Sas","n":1}}},{"i":3,"$":{"0":{"v":"Visual Investigator","n":0.707},"1":{"v":"\nSAS course: **LWVIA105**\n\n### Key features\n- Search and discover\n- Surveillance engine\n- Alert/Event Management\n- Case (investigation) management\n- SAS Mobile investigator 🤷‍♂️\n\n### Setup\nNeeds to be configured alongside admins and SAS consultants. Course focus on end user (investigator).\n\n## Alerts\n- Ability to manage alerts - alerts etc must be configured by **Solution Admin**\n- Each alert contains entities and mini-network diagram\n- alerts appear based on an accumulating risk score\n- ability to claim and manage an alert\n- possible to add attachments and comments to an alert\n- view history, risk scores, network diagram etc\n- An investigation can be created from an alert\n- Can use maps, timelines, network diagrams for viewing relationships\n\n## Investigations\n- Investigations can be claimed by investigators\n- Investigations can be sent between manager and investigator, for example\n- Workflow can be adapted to any situation e.g. send to another team\n- Extra information can be attached to investigations, same as alerts\n\n### Workspaces\nWorkspaces provide an area for visualising the data for an alert/investigation, which is linked to the entity (or entities). The following are provided ways to visualise data:\n- Detailed and tabular views\n- Maps\n- Network (entity to documents)\n- Timeline view\n\nNetworks can be edited and snapshots taken, creating multiple workspaces if one so wishes.\n\n### Search\n- allows ability to perform advanced searches\n- Possible to view on a map view, where relevant\n- investigations can be created from searches\n- limited to searching against individual object types\n\n## Reporting\n- Management report: Alert reports, task reports, audit reports\n- Not currently possible to create individual investigation/alert reports\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","n":0.065}}},{"i":4,"$":{"0":{"v":"DI Studio","n":0.707},"1":{"v":"\n# Introduction\n\n- DI Studio is still a SAS 9 platform\n- Possible SAS vIYA version of DI Studio\n\n## SAS Platform Architecture\n\n- **Clients:** SAS EG and VIYA\n- **Middle tier:** all web accessed tools and services\n- **SAS Servers:** SAS session running in the background\n- **Data Sources:** Any data, in any format\n\nMake use of meta data server and workspace server to run some of the DI Studio jobs. Preparing the data for the analysts.\n\n## Metadata\n- All about the metadata\n- Must be registered as a piece of data or job or process etc\n- types of Metadata:\n    - tables\n    - jobs\n    - libraries (to access tables)\n    - users\n    - etc...\n- All stored in SAS folders, which we can create to store metadata\n- Can create _SAS package_ to store metadata objects\n\n## Core primary tasks\n1. Define metadata for source data (register required source tables)\n2. Define metadata for target tables (input)\n3. Define jobs (build jobs using input tables)\n\n## Course Star Schema\n![](/assets/images/2022-02-07-10-25-59.png)\n\n## Change management\n- All using the same folder so working in same folders at the same time\n- Possibility of overwriting changes by others\n- change management comes in here\n- allows us to lock metadata, by checking out any metadata\n- Use checkouts tab represents own personal repo\n- To load stuff into checkout folder, drag and drop metadata into `Checkouts`\n- Tick mark will appear next to metadata\n- Check back in once changes made\n- Note: Not version control\n- Ensure metadata is checked back in once complete, or cleared\n\n# Creating metadata for source data\nNote: the process is very similar for varying data sources e.g. Oracle, CAS, Hadoop, Flat files etc\n\n1. Create new folder, specifically for the project source data (good practise)\n2. Right click folder and create `New` > `library...`\n3. Right click folder and click `Register Tables...`\n4. Repeat accordingly\n\n![Example: source data](/assets/images/2022-02-07-13-44-49.png)\n\n*Example: Source Data*\n\n\n# Creating metadata for target data\n1. Create new folder, specifically for the project target data (good practise)\n2. Right click folder and create `New` > `Table...`\n3. Can define library within the `New Table` wizard\n4. Columns can be defined manually or from existing data\n5. At this stage indices can also be defined\n4. Repeat accordingly\n\n![Example: target data](/assets/images/2022-02-07-15-35-42.png)\n\n*Example: Target Data*\n\n\n## Importing metadata\nIndustry standard is **Common Warehouse Metamodel format (CWM)**, whereas SAS package is SAS specific internal storage solution for metadata.\n\n1. Right click folder and select `Import` > `Metadata...`\n2. Select appropriate CWM (usually XML file)\n3. Change data definitions as appropriate, ensuring names, formats etc are correct\n\n\n# Creating metadata for jobs\n\n![Example Process Flow](/assets/images/2022-02-07-16-04-22.png)\n\n*Example Process Flow*\n\n1. Create new folder, specifically for the job metadata (good practise)\n2. Right click folder and create `New` > `Job...`\n3. Create a job flow accordingly, dragging in source/target data and transformations\n4. Test by running & click save\n\n- Other jobs can be dragged into a new job to create a process flow\n- Can use `Control Flow` tab to prioritise order of jobs\n\n## Intermediate tables\n- Rename `Physical name` using the `Physical Storage` tab\n- Ensure intermediate tables are data views if only used once (memory saving)\n\n### Transformation: Splitter\nSplits a table according to `Selection Conditions` in `Row Selection` tab\n\n### Transformation: File Reader\nFor reading in flat files based on metadata\n\n### Transformation: Table Loader\nFor column selection and how to feed data into target data e.g. insert, append or replace\n\n### Transformation: Join\n- Supports SQL pass-through\n- Allows for fine tuning the individual clauses of the SQL query\n- For **inner join** only must use where clause to indicate where columns match\n- Must ensure all data is mapped on the `Select` clause\n- Computed columns can be defined in the `Select` clause\n\n### Transformation: Extract\nFor applying filters, grouping or/and sorting\n\n### Transformation: Summary Statistics\nFor applying tailored summary statistics such as averages and percentiles etc, to be output as a report or as data\n\n### Transformation: Data Validation\n- Used to identify invalid, missing or duplicate values\n- Has ability to `Move row to error table`, `Change value to` or `Abort job`\n- Outputs a table of valid rows, invalid rows and an exceptions report\n\n\n# Additional features of jobs\n\n## Propagation & Mapping\n- Copying and pasting of Metadata definitions\n- Automatic propagation only occurs for temporary tables (can be switched off)\n\n!Propagation Tools](/assets/images/2022-02-08-14-12-16.png)\n\n*Propagation Tools*\n\n- Numeric/character conversions will occur automatically\n- Automatic mapping will occur if length is not less\n- Automatic & manual propagation can be defined at transformations/global/job level\n- Can use `PROC Metalib` to update Metadata to match source data\n\n## Status\n- Dependent on status of job, can output message or add log to data\n- Can be found in properties of transformations and jobs\n- Messages are so far impenetrable to code injection\n\n## Importing SAS code\nSAS code wizard:\n- Executes the programs\n- Analyses the results\n- Saves analysis to a file\n\nDo the following:\n1. Right click folder where want to import SAS code\n2. Click `Import...` > `Import SAS Code`\n3. Follow wizard and select SAS9 code\n\nRecommended to test first in DI Studio, using `Tools` > `Code Editor`, then `File` > `Open` > `SAS Code`.\n\n## Outputting to CAS/VIYA\n1. Create library to connect to existing CAS library & database\n2. Use `Cloud Analytics Services Transfer` transformation in a job\n3. Specify the CAS library to output data to\n\n## Impact Analysis\n- Can right click `Analyze` to review dependencies\n- Reveal where a metadata object is dependent on other metadata objects\n- Reveal where an metadata object is depended upon by other metadata objects\n\n# The Scheduling Process\nRequirements Scheduling Manager to schedule jobs and scheduling server to create triggers.\n\n(Similar to AWS Step functions or batching and Eventbridge for triggering)\n\nIn this case the `SAS application` is **DI Studio**.\n\n![](/assets/images/2022-02-09-14-43-00.png)\n\n*https://documentation.sas.com/doc/en/bicdc/9.4/scheduleug/n1f7xolpdktgjkn18aiem5lchkxl.htm*\n\n## Steps\n\nRight click the job to schedule and click `Scheduling` > `Deploy...`\n\n![](/assets/images/2022-02-09-14-48-22.png)\n\n\nOnce complete a new job will appear in the specified folder, with scheduled job icon\n\n![](/assets/images/2022-02-09-14-49-25.png)\n\n\nThis can be done in batch by highlighting multiple jobs at once\n\n![](/assets/images/2022-02-09-14-51-16.png)\n\n\nNavigate to SAS Management Console\n\n![](/assets/images/2022-02-09-14-53-34.png)\n\n\nRight click`Schedule Manager` > `New Flow...`\n\n![](/assets/images/2022-02-09-14-55-16.png)\n\n\nAdd OR/AND gates to define dependencies of jobs\n\n![](/assets/images/2022-02-09-14-56-09.png)\n\n![](/assets/images/2022-02-09-14-58-32.png)\n\nOnce complete right click schedule job and click `Schedule Flow...`\n\n![](/assets/images/2022-02-09-15-00-42.png)\n\n![](/assets/images/2022-02-09-15-01-18.png)\n\n\nFinally navigate to scheduling server to schedule (e.g. IBM Flow Manager)\n\n![](/assets/images/2022-02-09-15-03-10.png)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","n":0.033}}},{"i":5,"$":{"0":{"v":"Python","n":1}}},{"i":6,"$":{"0":{"v":"XML","n":1},"1":{"v":"\n## How to check if an XML sitemap contain some urls\nhttps://stackoverflow.com/questions/15083200/how-to-check-if-the-sitemap-contain-some-urls\n\n\n```python\nfrom xml.etree import ElementTree as etree\n\nurlset = etree.fromstring(xml)\nif urlset.find('url') is None:\n   print(\"sitemap has no urls\") \n\n```\n\n\n","n":0.196}}},{"i":7,"$":{"0":{"v":"Testing","n":1}}},{"i":8,"$":{"0":{"v":"Pytest","n":1},"1":{"v":"\n## Parametrize\nTest multiple different inputs within the same test function/class.\n\n```python\n@pytest.mark.parametrize(\n\n    # argument names\n    [arg1_name, arg2_name, ...],  \n\n    # argument values\n    [\n        (arg1, arg2, ...),  # test 1\n        (arg1, arg2, ...),  # test 2\n        (arg1, arg2, ...),  # test 3\n    ]\n\n)\n```\n\n## Exceptions\nTests for exception when expected.\n```python\nwith pytest.raises(Exception):\n    test_func()\n```\n\n## Approximations\nUseful for floating point comparison.\n\n```python\nassert 6.6666666 == pytest.approx(6.6666)  # True\n```\n\n\n\n","n":0.132}}},{"i":9,"$":{"0":{"v":"Mocks","n":1},"1":{"v":"\n## Intro\nMocks provide a way to interact with external systems during testing, without needing access the external system.\n\n```python\nfrom unittest import mock\n\n# create mock object\nm = mock.Mock()\n\n# set an attribute\nm.some_attribute\n\n# set a function/method\nm.some_attribute()\n\n# set return value\nm.some_attribute.return_value = 42\nm.some_attribute()  # returns 42\n```\n\n## Complex return values (side_effect)\nHowever, the problem comes when trying to set a function as the return value to the attribute. \n\nExample:\n```python\n\n# any function\ndef print42():\n    return 42\n\n# set attribute return value to print42\nm.some_attribute.return_value = print42\n\nm.some_attribute() \n# call attribute from mocks - expecting the return value of 42\n# does not resolve print42 function and prints function meta data instead\n\n```\n\nTo resolve this issue, mock also has the `side_effect` attribute, which accepts callables, iterables, and exceptions, changing behaviour accordingly.\n\n- if an exception is passed the mock will raise it\n- if an iterable is passed the mock will yield the results of the iterable\n- if a callable is passed the mock will resolve it\n\n```python\nm.some_attribute.side_effect = range(3)\n\nm.some_attribute()  # 1\nm.some_attribute()  # 2\nm.some_attribute()  # 3\nm.some_attribute()  # StopIteration exception\n\n\n# any function\ndef print42():\n    return 42\n\nm.some_attribute.side_effect = print_answer\nm.some_attribute()  # 42\n```\n\n## Asserting calls\n\n```python\nclass MyObj():\n    def __init__(self, repo):\n        repo.connect()\n\nfrom unittest import mock\nimport myobj\n\ndef test_connect():\n    external_obj = mock.Mock()\n    myobj.MyObj(external_obj)\n    external_obj.connect.assert_called_with()\n```\n\nThis would pass the test as the mock in this case is simply recording what it was called with, using the `assert_called_with()` method. \n\nIt is also possible to pass the expected arguments e.g. `assert_called_with(t=10, x=\"string\")`. In the above example this would fail as the argument was actually called with no arguments. \n\nThere are other methods and attributes mock provides:\n- `assert_called_once_with`\n- `assert_any_call`\n- `assert_has_calls`\n- `assert_not_called`\n- `call_count`\n- etc...\n\n## Patching\nPatching essentially intercepts known function calls and replace the return value.\n\nThis can be useful when we know the value will change dependent on external factors, which do not depend on our code. The downside to patching is the test is no longer pure and relies on knowing upfront which callable return value will require patching.\n\n```python\nimport os\nfrom unittest.mock import patch\n\ndef filepath(relative_path):\n    return os.path.abspath(relative_path)\n\ndef test_filepath():\n    relative_path = '../somefile.ext'\n\n    # patch externally dependent callable and test within with block\n    with patch('os.path.abspath') as abspath_mock:\n\n        # fake absolute path\n        test_abspath = 'some/abs/path'\n\n        # set new output for os.path.abspath\n        abspath_mock.return_value = test_abspath\n\n        # when os.path.abspath called returns 'some/abs/path' instead\n        assert filepath(relative_path) == test_abspath\n```\n\n### Patching decorator\nPatching can alternatively be done with the help of a decorator.\n\n```python\n[...]\n\n# decorator applied to entire function\n@patch('os.path.abspath')\n\n# first argument is now the the mock variable\ndef test_filepath(abspath_mock):\n    relative_path = '../somefile.ext'\n\n    # with statement now removed\n    test_abspath = 'some/abs/path'\n    abspath_mock.return_value = test_abspath\n    assert filepath(relative_path) == test_abspath\n```\n\nSame can also be achieved for multiple patches at the same time.\n\n```python\n[...]\n\n# two decorators applied\n@patch('os.path.getsize')\n@patch('os.path.abspath')\n\n# second argument is added for the getsize mock\ndef test_filepath(abspath_mock, getsize_mock):\n    relative_path = '../somefile.ext'\n\n    test_abspath = 'some/abs/path'\n    abspath_mock.return_value = test_abspath\n\n    # fake size set as output for os.path.getsize\n    test_size = 1234\n    getsize_mock.return_value = test_size\n\n    assert filepath(relative_path) == test_abspath\n    assert filesize(relative_path) == test_size\n```\n\n\n### Immutable objects\nObjects implemented in C are shared between the interpreter and require the objects to be immutable. Therefore it is not always possible to directly patch an object.\n\nFor example, if we were to attempt to patch the datetime now method `@patch('datetime.datetime.now')`, this would throw an error `TypeError: can't set attributes of built-in/extension type 'datetime.datetime'`.\n\nThere are ways to address this and start from the fact that importing or sub-classing an immutable object gives you a mutable _copy_.\n\n```python\n# patch now points to imported module\n@patch('fileinfo.logger.datetime.datetime')\ndef test_log(mock_now):\n    test_now = 123\n    test_message = \"A test message\"\n    \n    # method \"now\" return value must be set at this point\n    mock_now.now.return_value = test_now\n    \n    test_logger = Logger()\n    test_logger.log(test_message)\n\n    assert test_logger.messages == [(test_now, test_message)]\n```\n\nCould alternatively create an additional function to call the immutable object and reference from that, however, this requires changing source code. Best practise to avoid changing source code for testing. \n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n","n":0.041}}},{"i":10,"$":{"0":{"v":"pip","n":1},"1":{"v":"\n## CLI\n```bash\n# view package details\npip show <package-name>\n\n# show all current packages installed in current environment\npip list\n\n# upgrade a package\npip install <package-name> --upgrade\n\n# uninstall a package\npip uninstall <package-name>\n\n# install from requirements file\npip install -r <requirements-file>\n\n# install from requirements file, to a target location\npip install -r requirements.txt -t /path/to/install\n```\n\n\n\n","n":0.146}}},{"i":11,"$":{"0":{"v":"os","n":1},"1":{"v":"\n```python\n# get filename from path\nos.path.basename('../book_list.txt')\n\n# get absolute path from relative path or filename\nos.path.abspath('../book_list.txt')\n``` \n","n":0.267}}},{"i":12,"$":{"0":{"v":"Functions","n":1},"1":{"v":"\n## Mutable arguments and default values\n\nhttps://thomas-cokelaer.info/tutorials/python/functions.html\n\nA mutable argument is evaluated and saved only when the function is defined. This means mutable arguments are not reset on each function call. In a lot of cases this is fine, but can result in unintended consequences if not careful.\n\nTo avoid this behaviour initialisation must be done with an immutable object:\n\n```python\ndef inplace(x, lst=None):\n    if lst is None: lst=[]\n        lst.append()\n    return lst\n```\nOr\n\n```python\noriginal = [0, 1, 2]\nfunction_changes_list(original[:])\noriginal\n[0, 1, 2]\n```\n\n","n":0.117}}},{"i":13,"$":{"0":{"v":"Conda","n":1},"1":{"v":"\n## Environments\n\n```bash\n# create new Conda environment\nconda create --name <name>\n\n# activate environment (Windows)\nconda activate <name>\n\n# activate environment (bash)\nsource activate <name>\n\n# deactivate current environment (Windows)\nconda deactivate\n\n# deactivate current environment (bash)\nsource conda deactivate\n\n# list all current environments\nconda env list\n\n# remove environment\nconda env remove --name <name>\n\n# export current environment (including env name) to YAML\nconda env export > environment.yaml\n\n# create environment from environment.yaml\nconda env create -f environment.yaml\n```\n\n## Installation\n\n```bash\n# install a package using conda-forge\nconda install -c conda-forge <package-name>\n```\n","n":0.118}}},{"i":14,"$":{"0":{"v":"Kedro","n":1}}},{"i":15,"$":{"0":{"v":"CLI","n":1},"1":{"v":"\n## Pipelines\n- kedro **pipeline create `pipeline-name`**\n- kedro **catalog create --pipeline `pipeline-name`**\n\n## Testing\n- kedro **test** (limited)\n- py.test _-svv_\n\n## Requirements\n- kedro **build-reqs**\n\n","n":0.224}}},{"i":16,"$":{"0":{"v":"Docker","n":1},"1":{"v":"\n## Overview\n_NetworkChuck_ (Youtube Channel)\n\nIn usual virtualisation, the operating systems are built on top of a hypervisor which is essentially virtualising the hardware.\n\nWith **Docker**, the an a _Docker Engine_ runs on top an existing operating system. In this case it is the OS which is virtualised, rather than the hardware. These run as isolated containers, are lightweight and fast.\n\n![](/assets/images/2022-01-16-08-59-02.png)\nhttps://www.youtube.com/watch?v=eGz9DS-aIeY\n\nThe reason why Docker containers are so fast is they share the kernel with the underlying operating system instead of rebuilding the kernel for each container. The drawback to this is a Linux Docker container will not run on a Windows OS and vice versa.\n\n## Basic Docker commands\n\n```bash\n\n# to launch a container with an OS of choice\ndocker pull centos\ndocker run -d -t --name <the-container-name> centos\n\n# view all current running containers\ndocker ps\n\n# enter into container and launch bash shell\ndocker exec -it <the-container-name> bash\n\n# stop & start a container\ndocker stop <the-container-name>\ndocker start <the-container-name>\n\n# view container performance: memory, CPU etc\ndocker stats\n\n# remove a container\ndocker rm <the-container-name>\n\n# login in to repo\ndocker login -u YOUR-USER-NAME\n\n# tag a container image to match repo\ndocker tag <the-container-name> YOUR-USER-NAME/<repo-name>\n\n# list all container ids\ndocker ps -aq\n\n# kill all running containers (bash)\ndocker stop $(docker ps -aq)\n\n# removal all containers\ndocker rm $(docker ps -aq)\n\n# cleanse docker, removing images & containers\ndocker system prune\n\n```\n\n## Building an app container\n1. Create a file called `Dockerfile` (typically in the root directory of a project)\n2. Fill `Dockerfile` with juicy instructions\n3. Build container image, using command `docker build -t <image-name> .`\n    - `t` flag tags the image\n    - the `.` tells Docker to look for the `Dockerfile` in the current directory\n4. Run docker container `docker run -dp 3000:3000 <image-name>`\n    - `d` flag is detached mode (in the background)\n    - `p` is referring to the port mapping, in this case `3000:3000`\n    \n\n\n\n\n\n\n\n\n\n","n":0.059}}},{"i":17,"$":{"0":{"v":"Wsl","n":1},"1":{"v":"\n## Overview\nWhen using Docker in Windows, WSL is responsible for allocating memory and CPU. By default there is no limit to the amount of memory which can be allocated. Over time memory allocation can creep up until there is barely anything left for the host OS.\n\n## Resolve WSL resource issue\nTo set the allocation limits to WSL:\n1. Navigate to User folder (C:/Users/user-name)\n2. Create blank `.wslconfig` file\n3. Add the following to the config file:\n    ```powershell   \n    [wsl2]\n    memory=2GB\n    processors=2\n    ```\n4. Open Powershell with admin rights\n5. execute command `Restart-Service LxssManager` (restarts WSL)\n\n## Other useful commands\n```bash\n# shutdown WSL\nwsl --shutdown\n```\n\n\n","n":0.103}}},{"i":18,"$":{"0":{"v":"Development","n":1}}},{"i":19,"$":{"0":{"v":"TDD","n":1},"1":{"v":"\n## TDD Rules\n_Clean Architecture in Python_\n\n- Test first, code later\n- Add bare minimum code required to pass tests\n- Shouldn't have more than one test failing at a time\n- Write code to pass test, then refactor\n- Test should fail first time ran, otherwise question whether test is needed\n- Never refactor code without tests in place\n- Start with best possible application program interface and work backwards\n\n## Gotchas\n_TDD, Where Did It All Go Wrong (Ian Cooper)_\n\n- Adding a new class is **not** the trigger for writing a test; the trigger is implementing a requirement\n- Tests must not be dependent on the implementation of other tests\n- Testing isolated methods creates tests which are hard to maintain\n- No test is coupled to implementation\n- Initially speed of implementation trumps design\n- Can't both understand the solution to the problem, and engineer the code right (can lead to over engineering or analysis paralysis)\n- Do not write any new tests when refactoring\n\n\n### Red, Green, Refactor\n_TDD, Where Did It All Go Wrong (Ian Cooper)_\n\n1. **Red:** Write a test that doesn't work to prove that the test will not always pass\n2. **Green:** Make the test work as quickly as possible (e.g. copy/paste from Stack Overflow)\n3. **Refactor:** Tidy up code and remove duplication\n\nAlternatively,\n1. Write test\n2. Run to ensure fails\n3. Make it run\n4. Remove duplication\n\n\n\n## Types of test\n_Clean Architecture in Python_\n\nFlow | Type | Description |Test?\n-----|------|------|------\nIncoming | Query | unit accepts arguments and returns some value | Yes\nIncoming | Command | unit accepts arguments and changes state of system | Yes\nPrivate | Query | unit interacts with internal unit to attain a value | Maybe\nPrivate | Command | unit interacts with internal unit to change state | Maybe\nOutgoing | Query | unit interacts with external component and gets response | Mock\nOutgoing | Command | unit changes the state of an external component | Mock\n\n## Patching\nIn order to independently test units of code which interact with external components, patching can be applied which _mocks_ aspects of the underlying code. The benefit being that this can be done without having to edit the source code, but you are required to understand the source code in order to implement patching. \n\nThis goes against the grain of TDD, so there are some useful guidance for implementing mocks and ensuring that unit and integration testing remain separate. Three _rules_ which help to determine whether the testing approach needs to be reviewed:\n\n1. An increasing number of patches e.g. > 3\n2. Overly complex patching e.g. several layers deep into class\n3. Consider patching like _hooks_ where each _hook_ is a step back from the perfect assumption.\n\n\n","n":0.049}}},{"i":20,"$":{"0":{"v":"Dendron","n":1}}},{"i":21,"$":{"0":{"v":"Templates","n":1}}},{"i":22,"$":{"0":{"v":"Meeting Notes Template","n":0.577},"1":{"v":"_Edit the [[dendron.templates.meet]] note to change this template generated for Dendron Meeting Notes._\n\n## Attendees\n<!-- Meeting attendees. If you prefix users with an '@', you can then optionally click Ctrl+Enter to create a note for that user. -->\n\n- @JohnDoe\n\n## Goals\n<!-- Main objectives of the meeting -->\n\n## Agenda\n<!-- Agenda to be covered in the meeting -->\n\n## Minutes\n<!-- Notes of discussion occurring during the meeting -->\n\n## Action Items\n<!-- You can add any follow up items here. If they require more detail, you can use `Create Task Note` to create each follow up item as a separate note. -->\n\n- Follow Up Task 1\n- Follow Up Task 2\n","n":0.099}}},{"i":23,"$":{"0":{"v":"Course","n":1}}},{"i":24,"$":{"0":{"v":"AWS CI/CD Pipelines","n":0.577},"1":{"v":"\nSource: **AWS for DevOps: Continuous Delivery and Process Automation** (Lynn Langit)\n\n![](/assets/images/2022-04-09-09-14-29.png)\n_Minimum pipeline for CI/CD_\n\n![](/assets/images/2022-04-09-09-15-40.png)\n_How a CI/CD pipeline would work in practise_\n\n![](/assets/images/2022-04-09-09-24-15.png)\n_Applying a CI/CD pipeline in AWS_\n\n![](/assets/images/2022-04-09-09-37-10.png)\n_Example of AWS CD Pipeline_\n\n![](/assets/images/2022-04-09-12-10-40.png)\n_Equivalent CI/CD tools in AWS_\n\n![](/assets/images/2022-04-09-17-42-54.png)\n_AWS Automation Tools and Services_","n":0.162}}},{"i":25,"$":{"0":{"v":"Cloud","n":1}}},{"i":26,"$":{"0":{"v":"Aws","n":1}}},{"i":27,"$":{"0":{"v":"Serverless","n":1},"1":{"v":"\n## Overview\nTypical application hosted on 3 tiers:\n1. Presentation layer (HTML,CSS,JS)\n2. Application layer\n3. Data layer\n\nEC2 instances could handle the presentation layer and the application layer, however this is not ideal and these tiers should be separated.\n\nPresentation layer can be hosted on S3, with static hosting enabled. JS can handle the dynamic nature of the front end if required.\n\nLambda can be used for backend functions e.g. CRUD operations. Could be one or more Lambda functions.\n\nAmazon API Gateway can be used as a front door to trigger the lambda functions, triggered by JS on the front end.\n\nAmazon Route53 can be used for domain name management and Amazon Cloudfront for caching. \n\nNetworking services not required for serverless applications e.g. VPC/subnets etc\n\n![](/assets/images/2022-02-17-21-58-07.png)\n\n*Example of application built in Serverless*\n\n","n":0.091}}},{"i":28,"$":{"0":{"v":"Networking","n":1},"1":{"v":"\n## Amazon VPC\nThe VPC is an isolated network created in the AWS cloud.\n\nMust choose three main factors:\n1. VPC name\n2. Region\n3. IP range (CIDR)\n\n### Subnet\nIn order to isolate services from one another, each service can be given a subnet within the VPC.\n\nThe purpose of the subnet is for access control or for network optimisation. In the most typical use case it's possible to create a public and private subnet, essentially preventing direct public access to certain services. \n\nMust specify the following, when setting up each subnet:\n1. VPC where subnet should reside\n2. Availability Zone (within the region)\n3. CIDR block, which must be subset of VPC CIDR\n\nTo maintain availability it is recommended to recreate the subnet within different AZs. \n\n#### Reserved IPs\n![](/assets/images/2022-01-15-13-16-12.png)\n\n## Gateway\nOnce the VPC is set up, a gateway (VIG) must be created to authenticate access. A Virtual Private Gateway (VPG) can also be created for securely connecting to another private network.\n\n## Routing\nA route table is created automatically when the VPC is setup. The route table determines the where network traffic is directed.\n\nThe default configuration is to allow traffic between all subnets within a local network. This cannot be deleted.\n\nIt is possible to also create custom tables for routing traffic from an internet gateway to a particular subnet. \n\n![](/assets/images/2022-01-17-19-43-38.png)\n\n## VPC security\n\n### Network ACL\n- Firewall at subnet level\n- control over what is allowed to enter or leave the subnet\n- default is to allow all traffic\n- must always consider outbound and inbound traffic\n\n### Security groups (EC2 instances)\n- Firewall for EC2 instance\n- default configuration is block all inbound traffic; allow all outbound traffic\n- open inbound ports to accept traffic from the internet\n\n\n## Example process setting up VPC for an Amazon EC2 instance\n1. Create elastic IP, for NAT gateway\n2. Create VPC with public & private subnets\n3. Create second set of public & private subnets for second AZ\n4. Set up route tables\n5. Set up security groups for EC2 instances\n6. launch EC2 instance with created VPC and public subnet\n\n\n\n    \n    \n    \n\n\n\n\n\n\n","n":0.056}}},{"i":29,"$":{"0":{"v":"Management","n":1},"1":{"v":"\n# Monitoring\nIn AWS many of the services create metrics, such as CPU usage and simultaneous connections. \n\nIn normal use a baseline can be established to help determine whether the services are running smoothly or not. \n\nCan trigger automated alerts to someone or something to remediate the issue, when metric deviate too far away from the baseline.\n\n## CloudWatch\nCollates all service statistics in one place.\n\nMost metrics are sent to CloudWatch from other AWS service automatically when the service is created.\n\n### Create a dashboard\nA dashboard is a customised page in the console which can be used to monitor different types of resources in a single view. Includes multi-region. \n\n1. Navigate to CloudWatch in AWS\n2. Click `Dashboards` > `Create dashboard`\n3. Provide a name and click `Create dashboard`\n4. Select from different available widgets to add to dashboard\n5. Select either `Metrics` or `Logs` for data source\n6. Select various metrics from different AWS services\n\nIt's also possible to create custom metrics which can be sent from an application. Could be very specific to use case, such as API calls. \n\n### Create an alarm\nAllows you to create threshold which can be triggered if a metric goes over for a period of time.\n\n1. Click `Alarms` > `In alarm` > `Create alarm`\n2. Select desired measure and click `Select metric`\n3. Edit the statistic for the alarm and period\n4. Edit the conditions and threshold for the alarm and click `Next`\n5. Select an alarm state trigger: `In alarm`, `OK` or `Insufficient data`\n6. Configure **SNS** for notifications\n7. Click `Next` and provide the alarm a name\n8. Click `Next` and preview & `Create alarm`\n\n# Optimisation\nRedundancy and autoscaling may be needed to ensure application always available, when issues with the server occur and when demand increases.\n\n- Can use second availability zone (AZ), if hosting an application\n- Automatic redirection of traffic is required\n- Can chose to scale vertically (capacity) or horizontally (increase simultaneous services)\n\n## Elastic load balancing (ELB)\n- Handles traffic to different EC2 instances\n- highly available and scalable\n- Regional service\n\n*Example of ELB directing traffic*\n\n![](/assets/images/2022-02-17-20-12-12.png)\n\n*Various load balancers*\n\n![](/assets/images/2022-02-17-20-14-34.png)\n\n### Application Load Balancer (ALB)\n- Listener port & protocol e.g. 80 HTTP\n- Target group is backend to direct traffic and ability to health check\n- Rules defines how requests are directed to targets (e.g. different parts of application)\n\n![](/assets/images/2022-02-17-20-26-02.png)\n\n#### Creating ALB\n- Can be created via EC2 service\n- Type can be selected at this point, along with listeners and VPC/Subnets\n- Security group can be configured to only allow certain traffic\n- Finally configure routing and targets\n\n## Auto-scaling\nUsing CloudWatch to monitor metrics, this can be used to trigger alert and subsequently trigger auto-scaling. This could involve increasing the number of EC2 instances.\n\n![](/assets/images/2022-02-17-21-07-12.png)\n\nIn order for EC2 instances to be auto-scaled:\n- A launch template must be created, which can be done from thh EC2 service in AWS console\n- An Auto Scaling group must then be created\n- ...selecting the launch template, VPC/Subnets, load balancer & target group\n- Enable ELB health checks\n- Select the minimum and maximum group size (number of total instances)\n- Enable target tracking to configure when to scale\n \n**Warning:** if auto scaling is active and EC2 instances are terminate, auto scaling will try to increase the number of EC2 instances back up to the minimum configured\n\n\n\n\n\n\n\n\n","n":0.044}}},{"i":30,"$":{"0":{"v":"Lambda","n":1},"1":{"v":"\n\nLambda is a serverless compute service for running code < 15 mins, on trigger.\n\n## Steps\n1. Package and upload code to Lambda function\n2. Configure trigger for when want function to run\n\nCharged only when Lambda function is running.\n\n# Layers\nhttps://www.youtube.com/watch?v=ebhcs-9FYJA_\n\nLayers provided a shared space (or layer) where additional python modules etc can be shared across Lambdas.\n\nA lambda can use up to five layers at a time. The total unzipped size of the function and all layers cannot exceed the unzipped **deployment** package size quota - 250 MB at time of writing, not to be confused with recent increase for **code** package to 10 GB. \n\n![](/assets/images/![](/assets/images/2022-04-23-10-47-39.png).png)\n\nLayers can provide multiple use cases:\n![](/assets/images/2022-04-23-11-20-44.png)\n\n\n## Setting up\nMust make a layers directory for module layer e.g. `lambda-layers/python/lib/python3.7/site-packages`\n\nCan create customs packages here, or:\n\n `pip install <module-name> --target lambda-layers/python/lib/python3.7/site-packages`\n\nCan also store reference data e.g. `lambda-layers/data/some-data.json`\n\nOnce installed or/and data added, need to zip folders, for reference in SAM template.yaml:\n\n`zip -r my-lambda-layer.zip data python`\n\n## Create lambda functions\nTo reference the modules, just call as normal. AWS will automatically pull these from defined path in the lambda deployment directory under the folder `opt`.\n\nTo call the reference data can use relative path `/opt/data/some-data.json`.\n\n## Editing the template.yaml\nLayers must be defined for each lambda, under properties:\n```yaml\nProperties:\n    Other: path\n    Stuff: etc\n    Layers:\n        - !Ref MyLambdaLayer\n```\n`!Ref` refers to the lambda layer definition:\n```yaml\nMyLambdaLayer:\n    Type: AWS::Serverless::LayerVersion\n    Properties:\n        LayerName: MyLambdaLayer\n        Description: What it contains\n        ContentUri: lambda-layers/my-lambda-layer.zip\n        CompatibleRuntimes:\n            - python3.6\n            - python3.7\n        LicenceInfo: MIT\n        RetentionPolicy: Retain  # can uss previous versions in lambda functions\n```\nNote: `ContentUri` is where path to layer zip file is specified.\n\nTo allow the layer to be used for any other lambda function in the future, can specify ARN in the template:\n```yaml\nOutputs:\n    MyLambdaLayerARN:\n        Value: !Ref MyLambdaLayer\n        Description: MyLambdaLayer ARN\n        Export:\n            Name: my-lambda-layer-arn\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","n":0.06}}},{"i":31,"$":{"0":{"v":"Global Infrastructure","n":0.707},"1":{"v":"\n## Regions\n\n![](/assets/images/2022-01-09-15-41-04.png)\n\n![](/assets/images/2022-01-09-15-55-58.png)\n\n\n### Choosing the right region\n- latency\n- pricing\n- service availability\n- compliance\n\n## Availability Zones\n\nWithin each region there are multiple zones (AZs). Each zone contains one or more data centers. \n\n![](/assets/images/2022-01-09-15-59-26.png)\n\nThis allows for redundancy at the data center **and** AZ level. \n","n":0.158}}},{"i":32,"$":{"0":{"v":"Deploy","n":1}}},{"i":33,"$":{"0":{"v":"Batch","n":1},"1":{"v":"\n# Deploy kedro to batch (simple version)\nThis deployment guide will outline the instructions for uploading a Kedro project to AWS Batch. This is for the main intention of allowing us to trigger running the project automatically using AWS Eventbridge, and output the results to an S3 bucket which can be accessed directly online.\n\nNote: assumes AWS account has been setup and configured\n\n### Future improvements\n- Use CDK/CLI to automate build of the AWS stack\n- Pass AWS parameters for better flexibility and separation of dependency\n- Run individual nodes on separate batch jobs\n- Improve public access policy for S3 bucket\n- Migrate to AWS serverless (step functions & lambda)\n\n## Create a new S3 bucket in AWS\n1. Navigate to S3 Management Console in AWS\n2. Click **Create bucket** \n3. Name the bucket, adhering to [bucket naming guidance](https://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html)\n4. Ensure the region selected is correct for the project\n5. Uncheck **Block all public access** so individual files can be exposed to the internet\n6. Leave the remaining settings as default and click **Create bucket**\n\n## Create a new Kedro config folder and catalog yaml\n1. Navigate to project root directory\n2. Create folder called `aws_batch` in `conf`\n3. Copy the `catalog.yml` from `base` and paste into `aws_batch`\n4. Within the copied `catalog.yml` edit the filepaths to point to the earlier created S3 bucket, replacing the data folder prefix with `s3://<bucket-name>/`\n\n## Ensure requirements.txt is up to date\n1. Ensure `src/requirements.txt` contains all the required packages (and correct versions) to run the project locally\n2. Can rebuild new virtual environment to test this is the case\n3. Finally add `s3fs>=0.3.0,<0.5` to requirements for reading/saving to S3 bucket\n\n## Create Docker container\n1. Ensure Docker Daemon is running\n2. Install **kedro-docker** using `pip install kedro-docker`\n3. run `kedro docker init` to create required files\n4. run `kedro docker build ` (note: repo name must be lowercase)\n\n## Push Docker image to repo (in this case AWS ECR)\n1. Set up the repo in AWS ECR, giving the repo a sensible name _e.g. investing-batch_\n2. From command line on local machine run: `docker tag <local-repo-name>:latest <AWS-account-ID>.dkr.ecr.<region>.amazonaws.com/<aws-repo-name>:latest`\n3. Login to AWS on local machine by running: `aws ecr get-login-password --region <region> | docker login --username AWS --password-stdin <AWS-account-ID>.dkr.ecr.<region>.amazonaws.com`\n4. Push the docker image to ECR: `docker push <AWS-account-ID>.dkr.ecr.<region>.amazonaws.com/<aws-repo-name>:latest`\n\n## Setup AWS Batch\n1. Create IAM role so Batch can share information with ECR and S3 [similar to this](https://aws.amazon.com/blogs/compute/creating-a-simple-fetch-and-run-aws-batch-job/)\n2. `Create job Definition` providing an appropriate name, assign the created IAM role, set command field to `kedro run --env aws_batch` and edit execution time etc accordingly, leaving most defaults as is\n3. `Create compute environment` providing an appropriate name and choosing Fargate \n4. `Create job queue` providing an appropriate name, the compute environment and setting the priority\n5. Submit a new job to test it is working as intended\n\n## Setup scheduling\n1. Navigate to AWS EventBridge\n2. Click `Rules` > `Create rule` and provide an appropriate name\n3. Select trigger type under `Define pattern` select either `Event` or `Schedule` driven\n4. Select `Batch job queue` under `Target` and fill in ARNs for Batch components\n5. Click `Create` to complete rule\n\n## Accessing the output data\n1. Navigate to S3 bucket\n2. Select data or output would like to access publicly\n3. Select `Actions` > `Make public using ACL`\n4. Output data can now be accessed any time using S3 URL\n\n## References\nhttps://kedro.readthedocs.io/en/latest/10_deployment/07_aws_batch.html\nhttps://aws.amazon.com/blogs/compute/creating-a-simple-fetch-and-run-aws-batch-job/\nhttps://docs.aws.amazon.com/AmazonS3/latest/userguide/bucketnamingrules.html\n","n":0.044}}},{"i":34,"$":{"0":{"v":"Databases","n":1},"1":{"v":"\n![](/assets/images/2022-02-15-07-39-05.png)\n\n_Database hosted on AWS managed database service (e.g. RDS/DynamoDB)_\n\n\n## Available databases\n![](/assets/images/2022-02-16-08-12-58.png)\n\n\n## AWS RDS\n\n### Supported database engines\n- Amazon Aurora\n- MySQL\n- MariaDB\n- PostgreSQL\n- Oracle\n- Microsoft SQL Server\n\n**Amazon Aurora** is highly optimised for AWS, with greater performance and durability. It is also compatibility with MySQL and PostgreSQL. Overkill for simple database.\n\n\n### Setup\n- Once database engine selected, DB instance size can also be selected similar to options for EC2 instance\n- Also need to create a username and password for database access\n- AZ can be configured which handles auto-replication and failover all handled by AWS\n- Charges per instance run-time (per hour)\n\n\n## AWS DynamoDB\n- Serverless\n- Non-relational database (NoSQL)\n- Standalone tables\n- Great for storing key-value pairs or documents\n- Not bothered with relations between tables\n- Charged per usage and amount of data\n- data organised into items\n- Items have attributes\n- Performant and scalable\n- No rigid schema required\n\n### Setup\n- Very simple to set up as no schema required and auto scales\n- Just provide name of table and that's it\n- Any items added to the table can have any attributes\n\n\n\n\n\n\n\n\n\n","n":0.077}}},{"i":35,"$":{"0":{"v":"Containers","n":1},"1":{"v":"\nAWS provides a couple of container orchestration services:\n- ECS (Elastic Container Service)\n- EKS (Elastic Kubernetes Service)\n\nNecessary to help run and manage lots of containers.\n\n## AWS Fargate\nAWS Fargate is a serverless compute platform for ECS or EKS. This is as an alternative to running EC2 instance, removing the need to manage the intricacies of the EC2 instance and shifting the responsibility of patching and updates to Amazon (i.e. serverless)\n\n\n\n\n\n\n","n":0.121}}},{"i":36,"$":{"0":{"v":"IAM","n":1},"1":{"v":"\n## Overview\nIAM is used to manage access/policies for different users as well as roles.\n\n### IAM roles\nRoles are an important part of AWS and allow temporary access between different AWS services, which are produced programmatically. This access can be tightly controlled between different services and the actions which can be performed between those services. \n\n## Recommendations\n- MFA\n- Separate IAM Users\n- IAM user groups to control access for individual users\n- controlled IAM roles for API calls betwen AWS services\n\n\n\n","n":0.114}}},{"i":37,"$":{"0":{"v":"EC2","n":1},"1":{"v":"\nEC2 are virtual instances which can be spooled up very quickly and come in many different flavours, which have different costs and performance. \n\nInstances are a combination of virtual processors (vCPUs), memory, network, and, in some cases, instance storage and graphics processing units (GPUs).\n\nFlexibility allows you to very quickly create, terminate and upgrade an instance. \n\nThese instances can be started in an Availability Zone of choice. For high availability it is recommended to consider at least two EC2 instances in separate Availability Zones. \n\n## AWS Management Console\nThe following step can be performed to create an EC2 instance, using the management console.\n\n**Click EC2 > launch instance**\n\n- Step 1: choose AMI\n- Step 2: choose instance type e.g. size etc\n- Step 3: Configure instance details, including **User data** script\n- Step 4: Add storage\n- Step 5: Add tags (key/value), to categorise instance for organisation\n- Step 6: Configure security group, to enable traffic required by the server (SSH, HTTP etc)\n\nA key pair is required if accessing the instance from outside of the AWS management console.\n\n## EBS storage\n- Elastic Block Storage (EBS) is persistent storage which can attach to one or more EC2 instances. As they are separate, they persist when the EC2 instance goes down. \n- They can be backed up using _snapshots_\n- use cases:\n    - OS\n    - Databases\n    - Enterprise applications\n    - Throughput-intensive applications\n\n\n\n\n\n\n\n","n":0.067}}},{"i":38,"$":{"0":{"v":"Books","n":1}}},{"i":39,"$":{"0":{"v":"Shape Up","n":0.707},"1":{"v":"\nhttps://basecamp.com/shapeup/shape-up.pdf\n\n- key to project work and software development\n- Execution is not everything as things can be executed the wrong way, destroying morale etc\n- book explores what executing right looks like\n- based around a company called _Basecamp_, who have developed this\n- Not waterfall, agile or scrum\n\n## Intro\n- guide to how product development is done at _Basecamp_\n- and a toolbox of techniques to apply to own process\n- for anyone within a business\n\n### Pain points\n- projects go on and on\n- no time to think strategically about a product\n- inability to get things done quickly\n- information slipping through the cracks.\n\n> Switched from adhoc project lengths to repeating cycles. Took some experimentation to find the optimal length: 6 weeks.\n\n> **Shaping** used to describe the upfront work to set boundaries and reduce risk on projects before they were committed.\n\n### Six-week cycles\n- work in six-week cycles\n- long enough to get something meaningful build start to finish\n- short enough that the deadline looms from the start, so time used wisely\n- majority of features can be built within one six-week cycle\n- no micromanagement during this six-week cycle\n- agile uses 2 week sprints which are not long enough and too much planning overhead\n- 2 week cool-down follows, where project teams are free to work on what they want.\n\n### Shaping\n- work is **shaped** before handing it off to a team or individual\n- small senior group works in parallel, who define the key elements of a solution\n- projects defined at right level of abstraction\n- focus less on estimates and more on appetite: How much time do we want to spend? How much is the idea worth?\n\n> This is the task of shaping: narrowing down the \nproblem and designing the outline of a solution that fits within the \nconstraints of our appetite.\n\n### Making teams responsible\n- full responsiblity given to teams/individuals\n- when teams more automomous, senior people can spend less time managing\n- with less time managing, more time can be spent **shaping up** better projects\n- When projects are better shaped, teams have clearer boundaries and work more autonomously\n\n### Targeting risk\n- Risk of: \n    - not shipping on time\n    - getting stuck\n    - getting bogged down with old projects\n    - wasting time on unexpected problems\n    - not having future capacity\n- Reduce risk in the shaping process by solving open questions before comitting project to a time box \n- Don't give project to a team or individual who is still in a rabbit hole or has tangled interdependencies\n- Risk reduced by capping to six weeks\n- If project runs over it doesn't get an extension. Artificial cap ensures time is not overly invested on a concept which needs rethinking first.\n\n\n# Shaping\n\n![](/assets/images/2022-01-13-08-14-04.png)\n\n## Principles of shaping\nDo so at right level of abstraction: not too vague, not too concrete\n\n1. It's rough\n2. The overall solution is solved: open questions/rabbit holes are already resolved\n3. It's bounded: indicates what is out of scope\n\n## Who shapes?\n- Requires combining interface ideas, with technical possibilities, with business priorities\n- Usually need collaboration\n- Also requires strategy: why does it matter? what does success look like? what is the cost?\n\n## Two tracks\n- Not possible to schedule _shaping_ work\n- Have two tracks: one for shaping, one for building (6 week)\n- During 6 week cycles teams are building what has already been shaped and shapers are working on future cycles\n- Work on the shaping track is kept private until committed to project/product, to give option to put pause or drop work when it's not working\n\n## Steps to shaping\n### 1. Set boundaries\n- how much time is the raw idea worth\n- how to define the problem\n\nFirst set boundaries on what is being done. It's important not to go into solution mode before ensuring the scope is set, at least in broad terms.\n\n#### Appetite\nSometimes an idea  gets us excited right away, in which case the excitement needs to be tempered. Should check whether this is something really able to invest time in, before committing resource or hours of long meetings.\n\nOther ideas might be boring. We have been asked to do something we don't particularly want to do. \n\nAppetite split into two groups:\n- Small batch, requiring 1/2 staff 1/2 weeks batched together in 6 week cycles\n- Large batch, requiring the entire 6 week cycle\n\nAll projects which don't fit into 6 weeks are broken down or problem narrowed first.\n\n#### Fixed time, variable scope\n- Appetite used to vary the scope of the project or work\n- fixed time determined by the 6 week (or couple week constraint)\n\nThis _fixed time, variable scope_ principle is applied at each stage of the process, including the shaping process.\n\n- the _appetite_ defines the overarching solution\n- the _fixed time box_ pushes the decision as to what is core to the project\n\n#### Narrowing the problem\n- Always important to fully understand the problem\n- Appetite can tell us how much research into the problem we should do\n\nThe boundary is set once the following three have been achieved:\n- [ ] raw idea\n- [ ] appetite\n- [ ] narrow problem definition\n\n\n### 2. Rough out the elements\n- Sketch out solution at high level\n- output is idea which solves the problem, but without fine details worked out\n- Get from ideas in words to the elements of a software solution\n- Move fast and cover lots of ideas\n\nAt this stage it is important that the design choices lead to more refined scope and boundary, and not as the intended design. It is also undecided whether the idea is worth committing resource to at this stage and the outcome of this will help to shape that.\n\n#### Move at the right speed\n- Either work alone or with experience/trusted partner who can keep pace\n- Avoid wrong level of detail: concrete enough to make progress on a solution, without getting dragged down into the details\n- Work by hand and explore the pros and cons quickly\n\nQuestions to answer:\n- [ ] Where in the current system does the new thing fit?\n- [ ] How do you get to it?\n- [ ] What are key components or interactions?\n- [ ] Where does it take you?\n\n#### Breadboarding\nThree main components:\n1. Places\n2. Affordance\n3. Connection lines\n\n![](/assets/images/2022-03-15-08-22-25.png)\nIn this case _Invoice_ and _Set up autopay_ are the places and _Turn on autopay_ is the affordance.\n\nThis allows us ot move quickly through ideas and confront problems we may have not originally thought of, with little distraction.\n\n#### Fat marker sketches\nThe purpose of this is to use a large pen size to create a rough visual design. The large pen size prevents the ability to go into detail on particular aspects of the design.\n\nThis ensures not jumping ahead too much but also encourages us to review problems which haven't initially been thought of.\n\nThe output of these sketches is a list of elements, which are specific and narrow.\n\n\n### 3. Address risks & rabbit holes\n> A shaped project should be free as holes as possible, but we only have a limited amount of time to shape the project.\n\n- Amend solution accordingly\n- Cut things out\n- Specify details at certain tricky spots\n\nWell shaped work should look like a thin-tailed probability distribution, with a slight chance it could take an extra week.\n![](/assets/images/2022-03-16-08-32-28.png)\n\nWith technical unknowns, unsolved design problems and misunderstanding the project can take multiple times this.\n![](/assets/images/2022-03-16-08-34-53.png)\n\n> We want to remove the unknowns and tricky problems from the \nproject so that our probability is as thin-tailed as possible. That \nmeans a project with independent, well-understood parts that \nassemble together in known ways.\n\n#### Look for rabbit holes\n\n1. Walk through the solution in slow motion\n2. Question the viability of each part of the solution:\n    - Does this require new technical work we have never done before?\n    - Are we making any assumptions?\n    - Are there technical limitations not been considered?\n    - Is there a hard decision we need to settle in advance?\n    \n#### Declare out of bounds & cut back\n- Call out anything which is superfluous to original requirements\n- Explicitly state things which are out of scope and should not be worked on \n- Cut back on requirements if beyond core components\n- Cutting back as much as possible on additional elements directly reduces risk\n\n#### Present to technical experts\n- Present some components to a technical expert or third-party\n- Test assumptions and identify whether possible\n- Ensure get to bottom of whether possible within 6 weeks\n- Hunt for showstoppers which may have been overlooked\n- Present as an idea on whiteboard, as they may have idea to simplify or approach problem differently\n- Could lead to another round of shaping.\n\n\n### 4. Write the pitch\nAt this point we’re ready to make the transition from privately \nshaping and getting feedback from an inner-circle to presenting the \nidea at the betting table.\n\n#### Ingredients\n1. **Problem:** the raw idea\n2. **Appetite:** how much time we want to spend & constraints\n3. **Solution:** presentation of core elements\n4. **Rabbit holes:** call out potential areas of solution to avoid problems\n5. **No-gos:** Anything specifically excluded (what the solution won't be)\n\n#### Possible delivery\n1. Post proposal so individuals on the betting table can view the proposal\n2. Give time for stakeholders to comment on the pitch\n3. Amend and present the pitch formally\n\n\n# Betting\n\n## Bets, not backlogs\n\n### No Backlogs\nDon't have backlogs, as these can stack up and we know in some cases we will never have time for. Reduce time having to look at the same thing over and over again, and want to focus on projects right now **- could use time sector system here**.\n\n### A few potential bets\n- Prior to 6-week cycle a _betting meeting_ is held where stakeholders decided what to do at the next cycle\n- They look at pitches from the last 6 weeks, only \n- There are no backlogs to review or giant list of ideas\n- They should only be reviewing shaped proposals which are potential bets\n- If proposal rejected, no matter the reason, this DOES NOT get added to a backlog\n- Proposals can be lobbied again 6 weeks later.\n\n### Decentralised lists\nEveryone (including separate teams) can individually track items/projects, without the need for a central backlog or list. This can include pitches, bugs, requests, or anything they want to pursue independently. None of these should form an input into the better process.\n\nThis approach spreads out responsibility of tracking what to do next and regular infrequent 121s between teams can help cross-pollinate ideas for future proposals. This allows different departments  to advocate for what they think is important. \n\nThe overall advantage is everything reviewed is always relevant, timely and of the moment.\n\n### Important ideas come back\n- It's easy to overvalue ideas, but in truth, they are cheap\n- Really important ideas come back\n- Hearing something multiple times will bring it to the surface as important\n- This includes bugs or other software issues\n\n## The betting table\n_Which projects to schedule_\n\n### Resource & constraints\n- must be able to determine who is available and for how long\n- **When people are available at different times due to overlapping projects, project planning turns into a frustrating game of Calendar Tetris**\n- Working in cycles drastically reduces this problem\n- Must also consider the types of project and who is involved\n- Also needs to include QA within cycle\n- Up to allocated team to decide how to juggle time.\n\n### The betting table\n- Meeting held during cool-down, to decide what to work on during next cycle\n- Potential bets are either new from past 6 weeks, or someone specifically revived\n- No backlog; just a few good options\n- Betting table small e.g. CEO, senior programming and product strategist\n- Everyone seen and had chance to review proposals beforehand\n- 1-2 hours at most considering the proposals\n- Output of the call is cycle plan, with no two step process to get approval\n- Decisions are based on:\n    - who's available\n    - business priorities\n    - kind of work been on with lately\n- Once complete the output cannot be changed and is final\n- Buy in from the top is essential to making this process work\n\n### Meaning of a bet\n> Bets have a payout\n\n- Output should be something meaningful, with the pitch defining the payout\n- Bets are commitments exclusively to work on that thing with no interruptions\n- Most that can be lost is 6 weeks time\n\n### Uninterrupted time\n> It’s not really a bet if we say we’re dedicating six weeks but then allow a team to get pulled away to work on something else\n\n- Bet must be honoured\n- Team must **not** be interrupted or pulled away on other things\n- if something comes up, the maximum wait time is 6 weeks - team is not disrupted!\n- If that thing is important it can be bet on for the next cycle\n- Worst case it's a real crisis, but **true** crisis are rare.\n\n#### Resonates very strongly\n> When people ask for “just a few hours” or “just one day,” don’t be fooled. Momentum and progress are second-order things, like growth or acceleration. You can’t describe them with one point. You need an uninterrupted curve of points. When you pull someone away for one day to fix a bug or help a different team, you don’t just lose a day. You lose the momentum they built up and the time it will take to gain it back. **Losing the wrong hour can kill a day. Losing a day can kill a week**\n\n### Circuit breaker\n- If the team does not finish the work within the 6 weeks period, there is no extension\n- Removes risk of runaway project\n- The original bet was on 6 weeks and no more\n- Helps to reframe proposals better in the future\n- Motivates teams to take more ownership over projects\n\n### Bugs?\nBugs are no different to other pieces of work and rarely present a crisis.\n\nEither,\n1. Optionally fix a bug during 2 week cool-down period\n2. Propose large fixes at better table for 6 week sprint\n3. Have a bug smashing week once a year\n\n### Keeping the slate clean\n- Must have clean slate with each cycle, with no scraps of old work\n- It is crucial to maximise options in the future\n- Road map remains in our heads\n- No downside to keeping options open, and massive upside to being able to act on the unexpected\n- This includes projects which need to span multiple cycles.\n\n## Place your bets\nThere are two distinct types of bet:\n1. Improvement to an existing product\n2. Building a new product\n\n### Existing products\n- Following standard Shape Up process\n- Expect finished and shipped by end of 6-week cycle\n- analogous to crafting a piece of furniture for a house already built\n\n### New products\n- analogous to build the house around the furniture\n- Can be split into 3 main phases of work:\n    1. R&D mode\n    2. Production mode\n    3. Clean-up mode\n\n#### R&D mode\n> We have to learn what we want by building it\n\nAdjust for this in 3 ways:\n1. Instead of betting on well-shaped pitch, bet on time of clarifying key pieces of new idea\n2. Senior staff make up the design team - cannot delegate if don't know what want yourself & architectural designs will determine what's possible and large decisions may be required in real-time\n3. There is nothing to deliver at the end of the 6-week cycle, except possibly foundations to help shape future work\n\n#### Production mode\nOnce R&D cycles have sufficiently refined architecture and those decisions are settled, the structure is in place to bring in other teams to contribute.\n\nIn production mode:\n1. Shaping is deliberate again\n2. Any team can be given the work\n3. Completing features is the goal\n\nAt this point, the product is not launched, but is getting refined.\n\n#### Clean-up mode\n> In final phases before launching new product all structure goes out of window\n\nIn this stage all remaining bits need to be completed and finalised:\n1. There is no shaping, with leadership at the helm cutting away distractions and bringing focus to what's important\n2. There aren't clear team boundaries\n3. Work is released in small chunks\n\nClean-up should not last longer than two cycles.\n\n### Questions to ask at the betting table\n\n1. Does the problem matter?\n2. Is the appetite right?\n3. Is the solution attractive?\n4. Is it the right time?\n5. Are the right people available?\n\n### Kick-off message\nOnce complete the projects being worked on in the next cycle are communicated to everyone.\n\n# Building\n\n## Hand over responsibility\n> The way to really figure out what needs to be done is to start doing real work.\n\n- Hand over the project and not tasks\n- Full responsibility passed over to the team or individual carrying out the work\n- First few days the teams get acquainted with what has been asked\n- Team starts off with some imagined tasks, and begin exploring the problem\n- New tasks are created as the developers try to figure things out\n- QA/testing needs to be done within the cycle\n- Communication usually handled after deployment of project/work\n\n## Get one piece done\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","n":0.019}}},{"i":40,"$":{"0":{"v":"AWS","n":1}}},{"i":41,"$":{"0":{"v":"SAM","n":1}}},{"i":42,"$":{"0":{"v":"SAM Template","n":0.707},"1":{"v":"\n#### Sources\n* [Mastering the AWS Serverless Application Model (AWS SAM)](https://www.youtube.com/watch?v=QBBewrKR1qg)\n* [AWS SAM Template Developer Guide](https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-specification-resources-and-properties.html)\n\n\n# Overview\n\nUnder the hood SAM templates are being converted to Cloudformation templates.\n\nThe SAM templates offer a much cleaner and readable alternative to creating Cloudformation templates directly.\n\n# Sam Templates\n\n\n## Serverless Resources\n* AWS::Serverless::Function\n* AWS::Serverless::Api\n* AWS::Serverless::HttpApi\n* AWS::Serverless::SimpleTable\n* AWS::Serverless::LayerVersion\n* AWS::Serverless::Application\n* AWS::Serverless::StateMachine\n\n### AWS::Serverless::Function\n_Creates an AWS Lambda_\n\n* A lot of events can trigger the lambda\n* Policy can be selected here (or custom one created)\n\n```yaml\n# Example of policy for reading in a specified DynamoDB table\nPolicies:\n    - DynamoDBReadPolicy:\n        TableName: !Ref SampleTable\n```\n\n### AWS::Serverless::Api\n_Creates REST API_\n\n```yaml\n# Example of creating API & using OpenAPI to define API\nMyAPI:\n    Type: AWS::Serverless::Api\n    Properties:\n        StageName: prod\n        DefinitionUri: ./swagger.yml\n```\n\n### AWS::Serverless::HttpApi\n_Supercharged REST API_\n\nAllows more options and refinement, as well as being quicker etc.\n\n```yaml\n# Example of HTTP API\nHttpApi:\n    Type: AWS::Serverless::HttpApi\n    Properties:\n        CorsConfiguration:\n            AllowMethods:\n                - GET\n                - POST\n            AllowOrigins:\n                - https://localhost:8080\n```\n\n### AWS::Serverless::SimpleTable\n_Creates a DynamoDB table_\n\nCan specify the Primary Key, capacity & managed encryption.\n\n```yaml\n# Example of simple DynamoDB table\nMyTable:\n    Type: AWS::Serverless::SimpleTable\n```\n\n```yaml\n# Example of more complex DynamoDB table definition\nMyTable:\n    Type: AWS::Serverless::SimpleTable\n    Properties:\n        TableName: my-table\n        PrimaryKey:\n            Name: id\n            Type: string\n        ProvisionedThroughput:\n            ReadCapacityUnits: 5\n            WriteCapacityUnits: 5\n        Tags:\n            Department: Engineering\n            AppType: Serverless\n```\n\n### AWS::Serverless::LayerVersion\n_Creates Lambda Layer_\n\nNote: can use makefile to build the layer, rather than doing so manually!\n\n```yaml\nMyLayer:\n    Type: AWS::Serverless::LayerVersion\n    Properties: \n        LayerName: python-resources\n        Description: required third party modules for Python\n        ContentUri: MyLambdaLayer/layer.zip\n        CompatibleRuntimes:\n            - Python3.6\n        LicenseInfo: MIT\n        RetentionPolicy: Retain\n    Metadata:\n        BuildMethod: python3.6 | makefile\n```\n\n### AWS::Serverless::Application\n_Allows us to use a pre-built serverless application_\n\n```yaml\nMyApplication:\n    Type: AWS::Serverless::Application\n    Properties: \n        Location: https://s3.amasonaws.com/bucket/tmpl.yaml\n```\n            \n\n\n### AWS::Serverless::StateMachine\n_Creates AWS StepFunction_\n\n```yaml\n# Example of Step Function defintion\nMySampleStateMachine:\n    Type: AWS::Serverless::StateMachine\n    Properties:\n        DefinitionUri: sfn/MyStateMachine.asl.json\n        Role: arn:aws:iam::role/service-role/MyRole\n        DefinitionSubstitutions:\n            MyFunctionArn: !GetAtt MyFunction.Arn\n            MyDDBTable: !Ref TransactionTable   \n```\n\n## Globals\nThis is intended to add efficiency to building templates.\n\nDefine defaults across resources, by moving properties here which are repeated throughout.\n\n## Customs functions and parameters\nCan be used to create parametrised template.\n\n### Pseudo parameters\nCan be referenced from within SAM template:\n- AWS::AccountId\n- AWS::Region\n- AWS::StackId\n- AWS::StackName\n- AWS::UrlSuffix\n- etc...\n\n### Intrinsic functions\n- !GetAtt\n- !Ref\n- !Sub\n- !Split\n- !Join\n- etc...\n\n### Examples\n```yaml\n# Output endpoint for Amazon API Gateway\n!Sub http://${ServerlessHttpApi}.execute-api.${AWS::Region}.amazonaws.com\n```\n\n\n\n","n":0.056}}},{"i":43,"$":{"0":{"v":"SAM CLI","n":0.707},"1":{"v":"\n# Setup and common pipeline\n\n```bash\n# Check SAM version\nsam --version\n\n# Download a sample application\nsam init\n\n# Specify runtime & name\nsam init --runtime python3.7 --name sam-app\n\n# Bootstrap a CI/CD pipeline\nsam pipeline init --bootstrap\n\n# Build your application (from application root folder)\nsam build\n\n# Deploy application (guided arg for first time)\nsam deploy --guided\n\n# Validate template.yaml configuration file\nsam validate\n\n# Delete stack ( AWS CLI)\naws cloudformation delete-stack --stack-name sam-app --region region\n```\n\n# Local testing\n\n```bash\n# Host API locally\nsam local start-api\n\n# Send API request\ncurl http://127.0.0.1:3000/hello\n\n# Invoke Lambda function directly\nsam local invoke HelloWorldFunction -e events/event.json\n\n# Test API gateway\nsam local generate-event apigateway aws-proxy --body \"\" --path \"hello\" --method GET > api-event.json\n\n# Create an example API output & copy to clipboard\nsam local generate-event apigateway aws-proxy | clip\n\n# Create an example putting data in s3 bucket & copy to clipboard\nsam local generate-event s3 put | clip\n```\n\n# Debugging\n\n```bash\n# Review Cloudwatch logs (--tail continuously updates output)\nsam logs --stack-name sam-app --name HelloWorldFunction --tail\n```\n\n\n\n\n","n":0.083}}},{"i":44,"$":{"0":{"v":"Parameters","n":1},"1":{"v":"\n# Overview\nFor storing global parameters and API keys.\n\nThis can be achieved by manually entering these on the AWS console, under Systems Manager (SSM) - Parameter Store.\n\n## Retrieving parameters\n```python\n# create Systems Manager object\nSSM = boto3.client('ssm')\n\n# return JSON response with details of parameter\nresponse = SSM.get_parameter(Name='slack-api', WithDecryption=True)\n\n# extract value of parameter\nvalue = response['Parameters']['Value']\n```\n\n\n","n":0.141}}}]}
